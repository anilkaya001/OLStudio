<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"/>
<meta name="theme-color" content="#050505"/>

<title>Nash Strategy of Portfolio Managing</title>

<!-- Fonts and Math (Strictly Latin Modern) -->
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sugina-dev/latin-modern-web@1.0.1/style/latinmodern-roman.css" crossorigin="anonymous"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sugina-dev/latin-modern-web@1.0.1/style/latinmodern-sans.css" crossorigin="anonymous"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">

<style>
:root{
    color-scheme: dark;
    --font-main: "Latin Modern Roman", serif;
    --font-ui: "Latin Modern Sans", sans-serif;
    
    --bg: #050505;
    --border: rgba(255, 255, 255, 0.08);
    --silver: #e8e8ea;
    --silver-dim: rgba(232, 232, 234, 0.65);
    --gold: #D4AF37;
    --gold-soft: #F7E7CE;
}

body {
    margin: 0;
    background: var(--bg);
    color: var(--silver);
    font-family: var(--font-main);
    -webkit-font-smoothing: antialiased; 
    -moz-osx-font-smoothing: grayscale;
}

/* Base Content Container for Standalone Viewing */
.standalone-wrapper {
    max-width: 860px;
    margin: 0 auto;
    padding: 60px 20px;
}

/* Article Content Styling */
.content { 
    font-family: var(--font-main) !important;
    font-size: 1.22rem;
    line-height: 1.95;
    color: color-mix(in oklab, var(--silver) 94%, var(--silver-dim));
    text-align: justify;
    text-justify: inter-word;
    letter-spacing: 0.3px;
}

.content h2 { 
    font-family: var(--font-main) !important;
    color: var(--silver);
    line-height: 1.35;
    text-align: left;
    font-weight: normal;
    font-size: 2.1rem; 
    border-bottom: 1px solid var(--border); 
    padding-bottom: 0.8rem;
    margin-top: 4rem;
    margin-bottom: 1.5rem;
}

.content h3 { 
    font-family: var(--font-main) !important;
    font-size: 1.5rem; 
    font-style: italic; 
    color: var(--gold-soft);
    margin: 2.5rem 0 1.2rem;
    font-weight: normal;
}

.content p { margin: 0 0 2rem; }
.content p:first-of-type::first-letter {
    font-size: 3.8rem;
    line-height: 0.85;
    float: left;
    margin-right: 0.6rem;
    margin-top: 0.25rem;
    color: var(--gold);
    font-family: var(--font-main) !important;
}

/* Caption Styling */
.caption {
    font-family: var(--font-main) !important;
    font-size: 1.3rem;
    font-style: italic;
    color: color-mix(in oklab, var(--gold-soft) 85%, transparent);
    text-align: center;
    margin: 1rem auto 3rem auto;
    padding: 0 2rem;
    border-left: 2px solid var(--gold);
    border-right: 2px solid var(--gold);
}

/* Pure CSS Expandable Abstract Component */
.abstract-container {
    position: relative;
    margin: 2rem 0 1rem 0;
    padding: 2rem;
    background: rgba(255, 255, 255, 0.02);
    border-radius: 12px;
    border: 1px solid var(--border);
}

.abstract-text {
    font-family: var(--font-ui) !important;
    font-size: 1rem;
    line-height: 1.7;
    color: color-mix(in oklab, var(--silver) 85%, transparent);
    transition: all 0.5s cubic-bezier(0.25, 1, 0.5, 1);
    
    /* Default Collapsed State */
    display: -webkit-box;
    -webkit-line-clamp: 4;
    -webkit-box-orient: vertical;
    overflow: hidden;
    mask-image: linear-gradient(to bottom, black 40%, transparent 100%);
    -webkit-mask-image: linear-gradient(to bottom, black 40%, transparent 100%);
}

#abs-toggle { display: none; }

/* Expanded State Logic via Checkbox Hack */
#abs-toggle:checked ~ .abstract-text {
    -webkit-line-clamp: 100;
    mask-image: none;
    -webkit-mask-image: none;
}

.expand-btn-wrapper {
    display: flex;
    justify-content: flex-start;
    margin-top: 15px;
    position: relative;
    z-index: 10;
}

.expand-btn {
    background: var(--bg);
    border: 1px solid color-mix(in oklab, var(--gold) 35%, transparent);
    border-radius: 30px;
    color: var(--gold);
    font-family: var(--font-ui) !important;
    font-size: 0.75rem;
    font-weight: 800;
    letter-spacing: 1px;
    text-transform: uppercase;
    cursor: pointer;
    padding: 8px 20px;
    display: inline-flex;
    align-items: center;
    gap: 8px;
    box-shadow: 0 4px 15px rgba(0,0,0,0.2);
    transition: all 0.3s cubic-bezier(0.25, 1, 0.5, 1);
    user-select: none;
}

.expand-btn:hover {
    background: var(--gold);
    color: #000;
    border-color: var(--gold);
    transform: translateY(-2px);
    box-shadow: 0 8px 20px color-mix(in oklab, var(--gold) 35%, transparent);
}

.expand-btn svg {
    width: 16px; height: 16px;
    transition: transform 0.4s cubic-bezier(0.68, -0.55, 0.265, 1.55);
    stroke: currentColor;
}

/* Button Text and Icon Swapping */
.btn-text-collapse { display: none; }
#abs-toggle:checked ~ .expand-btn-wrapper .btn-text-expand { display: none; }
#abs-toggle:checked ~ .expand-btn-wrapper .btn-text-collapse { display: inline; }
#abs-toggle:checked ~ .expand-btn-wrapper svg { transform: rotate(180deg); }

/* Equation Styling */
.eq {
    margin: 1.8rem 0;
    padding: 22px 22px;
    border-radius: 12px;
    border: 1px solid rgba(128,128,128,0.15);
    background: rgba(0,0,0,0.22);
    overflow-x: auto;
    color: var(--silver);
    font-size: 1.05rem;
}

.eq-title {
    font-family: var(--font-ui) !important;
    font-size: 0.78rem;
    letter-spacing: 1.1px;
    text-transform: uppercase;
    color: var(--gold);
    margin-bottom: 10px;
}

/* Programming Code Blocks */
pre {
    background: rgba(15, 15, 18, 0.95);
    border: 1px solid var(--border);
    padding: 1.5rem;
    border-radius: 12px;
    overflow-x: auto;
    font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, monospace !important;
    font-size: 0.9rem;
    color: #d1d1d1;
    box-shadow: inset 0 2px 10px rgba(0,0,0,0.5);
    margin: 2.5rem 0;
    line-height: 1.5;
}

code {
    font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, monospace !important;
}

/* Python Syntax Highlighting Simulation */
.kw { color: #c678dd; } /* Keywords */
.fu { color: #61afef; } /* Functions */
.st { color: #98c379; } /* Strings */
.op { color: #56b6c2; } /* Operators */
.cm { color: #7f848e; font-style: italic; } /* Comments */
.nu { color: #d19a66; } /* Numbers */

</style>
</head>
<body>

<div class="standalone-wrapper">
    <div class="content">

        <div class="abstract-container">
            <input type="checkbox" id="abs-toggle">
            <div class="abstract-text">
                <strong>Note</strong> Here is a portfolio optimization setup I have been experimenting with recently. Instead of just blindly maximizing the Information Ratio I decided to structure the whole thing as a zero sum game. Maximizing a pure ratio is usually a terrible idea anyway since it lacks convexity and loves to chase noise. This new setup is basically you against a market adversary. You try to pick the best weights and the adversary actively tries to ruin your expected returns. If you solve for the saddle point you end up with a surprisingly stable Nash equilibrium. Better yet the math simplifies down into a basic quadratic program every month. I feed it exponentially weighted moments and shrink the covariance matrix to keep things grounded. To keep myself honest the backtest uses a strict walk forward loop with an embargo day to stop data leakage. It also applies proper turnover costs and uses some heavy duty statistical checks like the Deflated Sharpe Ratio.
            </div>
            <div class="expand-btn-wrapper">
                <label for="abs-toggle" class="expand-btn">
                    <span class="btn-text-expand">Read Full Abstract</span>
                    <span class="btn-text-collapse">Show Less</span>
                    <svg fill="none" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" d="M19 9l-7 7-7-7"></path></svg>
                </label>
            </div>
        </div>

        <div class="caption">
            A robust portfolio allocation framework designed to survive out of sample testing by assuming your return estimates are actively trying to destroy you.
        </div>

        <h2>1. The Mathematical Foundation</h2>
        
        <p>Note that none of this is investment advice. Let us dig deeply into the actual mathematics driving this setup. Standard mean variance optimization essentially operates as a ruthless error maximization machine. It aggressively overallocates capital to the specific assets that show the highest estimated returns combined with the lowest estimated variances. Naturally those extreme statistical estimates almost always contain the highest amounts of invisible market noise. You inevitably end up with massive concentrated positions in the exact areas where your historical data is the most hopelessly flawed. I built my framework to flip this entire destructive dynamic completely upside down. We begin by acknowledging our ignorance. We define our expected returns as a baseline historical estimate plus some unknown and highly hostile perturbation vector \(\delta\).</p>

        <div class="eq">
          <div class="eq-title">Equation 1 Zero Sum Game Formulation</div>
          \[
          \max_{w\in\mathcal{W}}\,\min_{\delta\in\mathbb{R}^N}\;
          (\hat{\mu}_t+\delta)^\top w
          -\frac{\gamma}{2}\,w^\top \hat{\Sigma}_t w
          +\frac{\eta}{2}\,\delta^\top \hat{\Sigma}_t^{-1}\delta
          -\tau\|w\|_2^2
          \]
          <p style="margin:10px 0 0 0; font-size:0.95rem; opacity:0.8; font-family: var(--font-ui) !important;">
            Here \(\gamma\) is your personal risk aversion parameter which acts as a fundamental active risk penalty. The variable \(\eta\) directly controls the strength of the adversary. A smaller value for \(\eta\) creates a far more aggressive and damaging opponent. The final term uses \(\tau\) to strictly penalize highly concentrated portfolio weights.
          </p>
        </div>

        <p>You essentially select your portfolio weights trying to get the absolute best risk adjusted return possible. At the exact same time your theoretical adversary is doing everything in their mathematical power to wreck those expected returns. The genius part is that the adversary is forced to use the local covariance structure as their weapon. Because the penalty against the adversary is purely quadratic we can actually find an exact closed form solution for their ultimate best response. You take the derivative of the inner minimization problem with respect to \(\delta\) and set it to zero. The math works out perfectly because our filtered covariance matrix is guaranteed to be positive definite. Plugging that optimal adversarial response back into your original equation simplifies the landscape beautifully. The intimidating minimax problem completely dissolves into a straightforward concave maximization formula.</p>

        <div class="eq">
          <div class="eq-title">Equation 2 The Reduced Saddle Point</div>
          \[
          w_t^\star=\arg\max_{w\in\mathcal{W}}\;
          \hat{\mu}_t^\top w
          -\frac{1}{2}\Big(\gamma + \frac{1}{\eta}\Big)\,w^\top\hat{\Sigma}_t w
          -\tau\|w\|_2^2
          \]
          <p style="margin:10px 0 0 0; font-size:0.95rem; opacity:0.8; font-family: var(--font-ui) !important;">
            Notice how the inverse of \(\eta\) effectively adds pure extra curvature directly to our risk penalty. This simple but profound mathematical trick forces the optimizer to act substantially more conservatively whenever the underlying market covariance grows dangerously large.
          </p>
        </div>

        <h2>2. Fetching Market Data Reliably</h2>
        
        <p>Grabbing historical price data from external web sources can turn into a massive nightmare during live execution. Open source APIs change their endpoint structures constantly and sometimes the adjusted close column simply disappears from the payload without any warning. I specifically wrote the primary fetch function to handle those exact edge cases gracefully. The code attempts to request the modern automatic adjustment feature first. If the local version of the library lacks that capability the script automatically catches the type error and instantly falls back to parsing the raw standard closing prices. This rigorous defensive logic ensures the data pipeline never snaps and ruins your automated monthly execution sequences.</p>

<pre><code><span class="kw">def</span> <span class="fu">fetch_prices</span>(tickers, benchmark, start_date=<span class="st">"2023-01-01"</span>, auto_adjust=<span class="kw">True</span>):
    symbols = <span class="fu">list</span>(<span class="fu">dict.fromkeys</span>(<span class="fu">list</span>(tickers) + [benchmark]))

    <span class="kw">try</span>:
        data = yf.download(
            symbols,
            start=start_date,
            progress=<span class="kw">False</span>,
            auto_adjust=auto_adjust,
            actions=<span class="kw">False</span>,
            group_by=<span class="st">"column"</span>,
            threads=<span class="kw">True</span>,
        )
        auto_adjust_effective = auto_adjust
    <span class="kw">except</span> <span class="fu">TypeError</span>:
        data = yf.download(
            symbols,
            start=start_date,
            progress=<span class="kw">False</span>,
            actions=<span class="kw">False</span>,
            group_by=<span class="st">"column"</span>,
            threads=<span class="kw">True</span>,
        )
        auto_adjust_effective = <span class="kw">False</span>

    <span class="kw">def</span> <span class="fu">_extract_field</span>(df, field):
        <span class="kw">if</span> <span class="fu">isinstance</span>(df.columns, pd.MultiIndex):
            <span class="kw">if</span> field <span class="kw">in</span> df.columns.get_level_values(<span class="nu">0</span>):
                <span class="kw">return</span> df[field].copy()
            <span class="kw">return None</span>
        <span class="kw">else</span>:
            <span class="kw">return</span> df[field].copy() <span class="kw">if</span> field <span class="kw">in</span> df.columns <span class="kw">else None</span>

    prices = <span class="kw">None</span>
    <span class="kw">if</span> auto_adjust_effective:
        prices = _extract_field(data, <span class="st">"Close"</span>)
    <span class="kw">else</span>:
        prices = _extract_field(data, <span class="st">"Adj Close"</span>)
        <span class="kw">if</span> prices <span class="kw">is None</span>:
            prices = _extract_field(data, <span class="st">"Close"</span>)

    <span class="kw">if</span> prices <span class="kw">is None</span>:
        <span class="kw">raise</span> <span class="fu">ValueError</span>(<span class="st">"Could not extract prices from yfinance output."</span>)

    prices.index = pd.to_datetime(prices.index)
    prices = prices[~prices.index.duplicated(keep=<span class="st">"last"</span>)].sort_index()
    prices = prices.replace([np.inf, -np.inf], np.nan)

    bench = prices[benchmark].dropna()
    assets = [t <span class="kw">for</span> t <span class="kw">in</span> tickers <span class="kw">if</span> t <span class="kw">in</span> prices.columns]
    port = prices[assets].copy()

    <span class="kw">return</span> port, bench</code></pre>

        <h2>3. Dynamic Moments and Covariance Shrinkage</h2>

        <p>Financial data structures never sit still for very long in the real world. A simple static average from five years ago tells you absolutely nothing about the complex market dynamics today. I choose to use an exponentially weighted moving average to calculate our vector inputs. You take your raw return matrix and apply a mathematically derived decay factor based on your desired span. This mechanism forces the internal model to care exponentially more about recent market regimes than ancient history. Dealing with the covariance matrix itself requires an entirely different layer of protection.</p>
        
        <p>Unfiltered sample covariance matrices act notoriously unstable when the number of assets begins to approach the number of time observations. The matrix frequently becomes poorly conditioned which makes reliable inversion practically impossible and destroys the optimizer. I apply a proven diagonal shrinkage technique directly to the raw covariance values. You take the noisy sample matrix and gently pull it toward a perfectly structured diagonal matrix. This vital stabilization step fundamentally stops the optimization engine from making extreme bets based on totally spurious historical correlations.</p>

<pre><code><span class="kw">def</span> <span class="fu">ewma_mean_cov</span>(X: np.ndarray, span: <span class="fu">int</span>):
    T, N = X.shape
    <span class="kw">if</span> T < <span class="nu">2</span>:
        <span class="kw">raise</span> <span class="fu">ValueError</span>(<span class="st">"Need at least 2 observations for covariance."</span>)

    alpha = <span class="nu">2.0</span> / (span + <span class="nu">1.0</span>)
    w = (<span class="nu">1.0</span> - alpha) ** np.arange(T - <span class="nu">1</span>, -<span class="nu">1</span>, -<span class="nu">1</span>)
    w /= w.sum()

    mu = w @ X
    Xm = X - mu
    cov = (Xm.T * w) @ Xm
    cov = <span class="nu">0.5</span> * (cov + cov.T)
    <span class="kw">return</span> mu, cov

<span class="kw">def</span> <span class="fu">shrink_cov</span>(cov: np.ndarray, shrink: <span class="fu">float</span>):
    s = <span class="fu">float</span>(np.clip(shrink, <span class="nu">0.0</span>, <span class="nu">1.0</span>))
    d = np.diag(np.diag(cov))
    <span class="kw">return</span> (<span class="nu">1.0</span> - s) * cov + s * d</code></pre>

        <h2>4. Implementing the Nash Solver Algorithm</h2>

        <p>Now we finally arrive at the core execution logic of the solver function. Because we brilliantly reduced the complex minimax problem down to a single equation earlier the implementation code becomes incredibly clean. You just need to define the objective function combining the expected returns with our artificially inflated risk penalty. I decided to pass that function directly into the SciPy library using the Sequential Least Squares Programming algorithm. I specifically chose the SLSQP method because it handles strict equality constraints exceptionally well. We desperately need our final output weights to sum exactly to one hundred percent. We also need to enforce hard boundary conditions so no single stock accidentally dominates the entire portfolio allocation. The algorithm rapidly descends to find the exact saddle point where neither the investor nor the adversary can profitably alter their strategy.</p>

<pre><code><span class="kw">def</span> <span class="fu">solve_nash_weights</span>(
    mu: np.ndarray,
    cov: np.ndarray,
    cap: <span class="fu">float</span> = <span class="nu">0.15</span>,
    l2: <span class="fu">float</span> = <span class="nu">1e-3</span>,
    risk_aversion: <span class="fu">float</span> = <span class="nu">8.0</span>,
    robustness_eta: <span class="fu">float</span> = <span class="nu">4.0</span>,
    maxiter: <span class="fu">int</span> = <span class="nu">3000</span>,
    tol: <span class="fu">float</span> = <span class="nu">1e-12</span>,
):
    mu = np.asarray(mu, <span class="fu">float</span>)
    cov = np.asarray(cov, <span class="fu">float</span>)
    N = <span class="fu">len</span>(mu)

    cov = <span class="nu">0.5</span> * (cov + cov.T)
    cov = cov + np.eye(N) * <span class="nu">1e-10</span>

    lam_eff = <span class="fu">float</span>(risk_aversion + <span class="nu">1.0</span> / robustness_eta)

    <span class="kw">def</span> <span class="fu">obj</span>(w):
        w = np.asarray(w, <span class="fu">float</span>)
        quad = w @ cov @ w
        <span class="kw">return</span> -(mu @ w - <span class="nu">0.5</span> * lam_eff * quad - l2 * (w @ w))

    cons = [{<span class="st">"type"</span>: <span class="st">"eq"</span>, <span class="st">"fun"</span>: <span class="kw">lambda</span> w: np.sum(w) - <span class="nu">1.0</span>}]
    bnds = <span class="fu">tuple</span>((<span class="nu">0.0</span>, <span class="fu">float</span>(cap)) <span class="kw">for</span> _ <span class="kw">in</span> <span class="fu">range</span>(N))
    w0 = np.full(N, <span class="nu">1.0</span> / N)

    res = sco.minimize(
        obj,
        w0,
        method=<span class="st">"SLSQP"</span>,
        bounds=bnds,
        constraints=cons,
        tol=tol,
        options={<span class="st">"maxiter"</span>: maxiter, <span class="st">"ftol"</span>: tol, <span class="st">"disp"</span>: <span class="kw">False</span>},
    )

    w = res.x <span class="kw">if</span> res.success <span class="kw">else</span> w0.copy()
    w = np.clip(w, <span class="nu">0.0</span>, cap)
    s = w.sum()
    w = (w / s) <span class="kw">if</span> s > <span class="nu">0</span> <span class="kw">else</span> w0.copy()
    <span class="kw">return</span> w, res</code></pre>

        <h2>5. Rigorous Statistical Validation</h2>

        <p>Evaluating strategy performance accurately poses immense difficulty for most researchers. Asset returns constantly exhibit fat tails and clump together in volatile clusters over time. Standard normal statistics blindly assume every single trading day operates independently. Betting real capital on that foolish assumption will inevitably bankrupt you. I integrated the Probabilistic Sharpe Ratio specifically to automatically adjust the performance score based on the underlying skewness and kurtosis found in the backtest logs.</p>
        
        <p>I also wrote a custom stationary bootstrap function to calculate our confidence intervals. Instead of just picking single random days from the past it selectively grabs random sequential blocks of daily returns. The length of each chosen block follows a strict geometric probability distribution. This advanced technique keeps the original market autocorrelation structures perfectly intact while generating highly realistic error bands for our final Information Ratio.</p>

<pre><code><span class="kw">def</span> <span class="fu">probabilistic_sharpe_ratio</span>(r, sr_star=<span class="nu">0.0</span>, periods=<span class="nu">252</span>):
    r = np.asarray(r, <span class="fu">float</span>)
    r = r[np.isfinite(r)]
    T = <span class="fu">len</span>(r)
    <span class="kw">if</span> T < <span class="nu">30</span>: <span class="kw">return</span> np.nan

    sr_hat = sharpe_ratio(r, periods=periods)
    <span class="kw">if not</span> np.isfinite(sr_hat): <span class="kw">return</span> np.nan

    g3 = st.skew(r, bias=<span class="kw">False</span>)
    g4 = st.kurtosis(r, fisher=<span class="kw">False</span>, bias=<span class="kw">False</span>)

    denom = np.sqrt((<span class="nu">1.0</span> - g3 * sr_hat + ((g4 - <span class="nu">1.0</span>) / <span class="nu">4.0</span>) * (sr_hat ** <span class="nu">2</span>)) / (T - <span class="nu">1.0</span>))
    <span class="kw">if</span> denom <= <span class="nu">0</span>: <span class="kw">return</span> np.nan

    z = (sr_hat - sr_star) / denom
    <span class="kw">return</span> <span class="fu">float</span>(st.norm.cdf(z))

<span class="kw">def</span> <span class="fu">stationary_bootstrap_ir</span>(active_r, n_boot=<span class="nu">500</span>, p=<span class="nu">0.10</span>, seed=<span class="nu">42</span>, periods=<span class="nu">252</span>):
    x = np.asarray(active_r, <span class="fu">float</span>)
    x = x[np.isfinite(x)]
    T = <span class="fu">len</span>(x)
    <span class="kw">if</span> T < <span class="nu">60</span>: <span class="kw">return</span> np.nan, (np.nan, np.nan)

    rng = np.random.default_rng(seed)
    ir_hat = info_ratio(x, periods=periods)

    irs = np.empty(n_boot, dtype=<span class="fu">float</span>)
    <span class="kw">for</span> b <span class="kw">in</span> <span class="fu">range</span>(n_boot):
        sample = np.empty(T, dtype=<span class="fu">float</span>)
        t = <span class="nu">0</span>
        <span class="kw">while</span> t < T:
            start = rng.integers(<span class="nu">0</span>, T)
            L = <span class="fu">int</span>(rng.geometric(p))
            end = <span class="fu">min</span>(T, start + L)
            chunk = x[start:end]
            k = <span class="fu">min</span>(<span class="fu">len</span>(chunk), T - t)
            sample[t:t+k] = chunk[:k]
            t += k
        irs[b] = info_ratio(sample, periods=periods)

    lo, hi = np.nanpercentile(irs, [<span class="nu">2.5</span>, <span class="nu">97.5</span>])
    <span class="kw">return</span> ir_hat, (<span class="fu">float</span>(lo), <span class="fu">float</span>(hi))</code></pre>

        <h2>6. A Hostile Testing Engine</h2>

        <p>You can invent the most mathematically elegant optimization model in human history but if your backtest lies to you it means absolutely nothing. That is exactly why the testing environment must remain completely ruthless. Standard open source backtesting frameworks almost always leak future data into the past eventually. I actively chose to build a custom execution loop from scratch to enforce a brutally strict walk forward testing process. The code manages an index of pure trading dates and only executes logical steps at the absolute end of the month.</p>
        
        <p>I also implement a deliberate gap logic known as an embargo. The engine intentionally lops off the final closing day of the training set before letting the optimizer see anything. Doing this permanently stops the final closing prices from magically bleeding across the validation boundary and inflating our theoretical returns. The loop then calculates portfolio turnover precisely by computing the absolute difference between the exact newly generated weights against the old active weights. It immediately subtracts a realistic basis point trading cost on the exact day the transition occurs. There is absolutely no hidden magic happening here. It just utilizes very careful chronological accounting and highly defensive logic.</p>

<pre><code><span class="kw">def</span> <span class="fu">run_backtest_nash</span>(
    port_prices: pd.DataFrame,
    bench_prices: pd.Series,
    lookback_days: <span class="fu">int</span> = <span class="nu">90</span>,
    embargo_days: <span class="fu">int</span> = <span class="nu">1</span>,
    ewma_span: <span class="fu">int</span> = <span class="nu">60</span>,
    cap: <span class="fu">float</span> = <span class="nu">0.15</span>,
    shrink: <span class="fu">float</span> = <span class="nu">0.10</span>,
    risk_aversion: <span class="fu">float</span> = <span class="nu">8.0</span>,
    robustness_eta: <span class="fu">float</span> = <span class="nu">4.0</span>,
    l2: <span class="fu">float</span> = <span class="nu">1e-3</span>,
    min_obs: <span class="fu">int</span> = <span class="nu">60</span>,
    min_coverage: <span class="fu">float</span> = <span class="nu">0.95</span>,
    tc_bps: <span class="fu">float</span> = <span class="nu">10.0</span>,
    dsr_trials: <span class="fu">int</span> = <span class="nu">1</span>,
    bootstrap_n: <span class="fu">int</span> = <span class="nu">500</span>,
    bootstrap_p: <span class="fu">float</span> = <span class="nu">0.10</span>,
):
    r_assets = port_prices.pct_change()
    r_bench = bench_prices.pct_change().rename(<span class="st">"BENCH"</span>)

    df = r_assets.join(r_bench, how=<span class="st">"inner"</span>).dropna(subset=[<span class="st">"BENCH"</span>])
    assets = <span class="fu">list</span>(port_prices.columns)

    eom = month_end_trading_dates(df.index)
    <span class="kw">if</span> <span class="fu">len</span>(eom) < <span class="nu">3</span>:
        <span class="kw">raise</span> <span class="fu">ValueError</span>(<span class="st">"Not enough months to run backtest."</span>)

    executed_weights = []
    executed_dates = []

    port_ret = []
    bench_ret = []
    active_ret = []
    idx_all = []

    prev_w = pd.Series(<span class="nu">0.0</span>, index=assets)
    turnover_hist = []

    <span class="kw">for</span> i <span class="kw">in</span> <span class="fu">range</span>(<span class="fu">len</span>(eom) - <span class="nu">1</span>):
        reb = eom[i]
        nxt = eom[i + <span class="nu">1</span>]
        <span class="kw">if</span> reb <span class="kw">not in</span> df.index <span class="kw">or</span> nxt <span class="kw">not in</span> df.index:
            <span class="kw">continue</span>

        reb_i = df.index.get_loc(reb)
        nxt_i = df.index.get_loc(nxt)

        <span class="cm"># Embargo day logic completely prevents boundary leakage</span>
        train_end = reb_i - <span class="fu">int</span>(embargo_days)
        <span class="kw">if</span> train_end <= <span class="nu">0</span>:
            <span class="kw">continue</span>
        train_start = <span class="fu">max</span>(<span class="nu">0</span>, train_end - <span class="fu">int</span>(lookback_days) + <span class="nu">1</span>)

        train = df.iloc[train_start : train_end + <span class="nu">1</span>]
        <span class="kw">if</span> <span class="fu">len</span>(train) < min_obs:
            <span class="kw">continue</span>

        A = train[assets].sub(train[<span class="st">"BENCH"</span>], axis=<span class="nu">0</span>)
        coverage = A.notna().mean()
        valid = coverage[coverage >= min_coverage].index.tolist()
        
        <span class="kw">if</span> <span class="fu">len</span>(valid) < <span class="nu">2</span>: <span class="kw">continue</span>

        A_valid = A[valid].dropna()
        <span class="kw">if</span> <span class="fu">len</span>(A_valid) < min_obs: <span class="kw">continue</span>

        X = A_valid.to_numpy(dtype=<span class="fu">float</span>)
        mu, cov = ewma_mean_cov(X, span=ewma_span)
        cov = shrink_cov(cov, shrink=shrink)

        w_valid, _ = solve_nash_weights(
            mu=mu, cov=cov, cap=cap, l2=l2,
            risk_aversion=risk_aversion, robustness_eta=robustness_eta,
        )

        w = pd.Series(<span class="nu">0.0</span>, index=assets)
        w.loc[valid] = w_valid

        executed_dates.append(reb)
        executed_weights.append(w.copy())

        turnover = <span class="fu">float</span>(np.abs(w - prev_w).sum())
        turnover_hist.append((reb, turnover))
        cost = (tc_bps / <span class="nu">1e4</span>) * turnover
        prev_w = w

        test = df.iloc[reb_i : nxt_i + <span class="nu">1</span>].iloc[<span class="nu">1</span>:]
        <span class="kw">if</span> <span class="fu">len</span>(test) == <span class="nu">0</span>: <span class="kw">continue</span>

        test_assets = test[assets].fillna(<span class="nu">0.0</span>).to_numpy(dtype=<span class="fu">float</span>)
        w_vec = w.to_numpy(dtype=<span class="fu">float</span>)

        pr = test_assets @ w_vec
        br = test[<span class="st">"BENCH"</span>].to_numpy(dtype=<span class="fu">float</span>)

        pr = pr.copy()
        pr[<span class="nu">0</span>] -= cost

        port_ret.extend(pr.tolist())
        bench_ret.extend(br.tolist())
        active_ret.extend((pr - br).tolist())
        idx_all.extend(test.index.tolist())

    <span class="kw">return</span> port_cum, bench_cum, avg_w, port_ret, bench_ret, active_ret, turnover_s, metrics</code></pre>

        <p>The compiled output ultimately flows into an entirely separate visualization block that creates a stark and mathematically honest view of the strategy over time. I consistently rely solely on the underlying hard data to defend the structural integrity of this Nash equilibrium model.</p>

    </div>
</div>

<!-- Math Rendering Initialization -->
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        if (typeof renderMathInElement === "function") {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "\\(", right: "\\)", display: false}
                ],
                throwOnError: false
            });
        }
    });
</script>
</body>
</html>
